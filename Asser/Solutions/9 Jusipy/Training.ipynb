{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from pandas.tools.plotting import parallel_coordinates\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "random_state = 100\n",
    "#import xgboost as xgb\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT DATA\n",
    "data = pickle.load( open( \"data/initial_features.pkl\", \"rb\" ))\n",
    "labels = pickle.load(open(\"data/initial_labels.pkl\", \"rb\"))\n",
    "\n",
    "\n",
    "#y = labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5338, 14) (5338, 1)\n",
      "(2288, 14) (2288, 1)\n"
     ]
    }
   ],
   "source": [
    "# create training and testing vars\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data, labels, test_size=0.3)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CV SVM train\n",
    "#clf = svm.SVC(kernel='linear', C=100)\n",
    "#scores = cross_val_score(clf, X_train, np.ravel(Y_train), cv=10)\n",
    "#print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "#scores = np.dot(X_train, clf.coef_.T)\n",
    "#b0 = Y_train==0 # boolean or \"mask\" index arrays\n",
    "#b1 = Y_train==1\n",
    "#malignant_scores = scores[b1]\n",
    "#benign_scores = scores[b0]\n",
    "\n",
    "#fig  = plt.figure()\n",
    "#fig.suptitle(\"score breakdown by classification\", fontsize=14, fontweight='bold')\n",
    "#score_box_plt = plt.boxplot(\n",
    "#    [malignant_scores, benign_scores],\n",
    "#    notch=True,\n",
    "#    labels=[0,1],\n",
    "#    vert=False\n",
    "#)\n",
    "#plt.show(score_box_plt)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = svm.SVC(kernel='linear')\n",
    "lin_clf.fit(X_train,Y_train)\n",
    "\n",
    "scores = np.dot(X_train, lin_clf.coef_.T)\n",
    "\n",
    "b0 = Y_train==0 # boolean or \"mask\" index arrays\n",
    "b1 = Y_train==1\n",
    "malignant_scores = scores[b1]\n",
    "benign_scores = scores[b0]\n",
    "\n",
    "fig  = plt.figure()\n",
    "fig.suptitle(\"score breakdown by classification\", fontsize=14, fontweight='bold')\n",
    "score_box_plt = plt.boxplot(\n",
    "    [malignant_scores, benign_scores],\n",
    "    notch=True,\n",
    "    labels=[0,1],\n",
    "    vert=False\n",
    ")\n",
    "plt.show(score_box_plt) \n",
    "\n",
    "# get the predictions\n",
    "probas_ = lin_clf.predict(X_test)\n",
    "            \n",
    "# turn the predictions into correct format\n",
    "probas_ = np.array([(1-x, x) for x in probas_])\n",
    "\n",
    "# create roc curve\n",
    "probs = lin_clf.predict(X_test)\n",
    "preds = probs\n",
    "fpr, tpr, threshold = metrics.roc_curve(Y_test, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# method I: plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get contributing features on (SVM) classification for a selected example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_example = data.values[3]\n",
    "\n",
    "lin_clf.predict(selected_example.reshape(1,14))\n",
    "\n",
    "contributions = np.multiply(selected_example, lin_clf.coef_.reshape((14,)))\n",
    "feature_number = np.arange(len(contributions)) +1\n",
    "\n",
    "features_coefficients = dict(zip(list(data), contributions))\n",
    "#print(features_coefficients)\n",
    "\n",
    "plt.bar(list(data), contributions, align='center')\n",
    "plt.xticks(list(data), rotation='vertical')\n",
    "plt.xlabel('feature index')\n",
    "plt.ylabel('score contribution')\n",
    "plt.title('contribution to classification outcome by feature index')\n",
    "plt.show()\n",
    "\n",
    "# Sort features\n",
    "\n",
    "#abs_contributions = np.flip(np.sort(np.absolute(contributions)), axis=0)\n",
    "#feat_and_contrib = []\n",
    "#for contrib in abs_contributions:\n",
    "#    if contrib not in contributions:\n",
    "#        contrib = -contrib\n",
    "#        feat = np.where(contributions == contrib)\n",
    "#        feat_and_contrib.append((feat[0][0], contrib))\n",
    "#    else:\n",
    "#        feat = np.where(contributions == contrib)\n",
    "#        feat_and_contrib.append((feat[0][0], contrib))\n",
    "\n",
    "# sorted by max abs value. each row a tuple:;(feature index, contrib)\n",
    "print(\"Sorted features by max value :\")\n",
    "#feat_and_contrib\n",
    "sorted(features_coefficients.items(), key=lambda t: t[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Stratified k-fold (k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores = []\n",
    "XX, YY = X_train, Y_train\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "#print(XX.iloc[50])\n",
    "for train_index, test_index in skf.split(XX, YY):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_ttrain, X_ttest = XX.iloc[train_index], XX.iloc[test_index]\n",
    "    y_ttrain, y_ttest = YY.iloc[train_index], YY.iloc[test_index]\n",
    "    svm_test = svm.SVC(kernel='linear')\n",
    "    svm_test.fit(X_ttrain,np.ravel(y_ttrain))\n",
    "    \n",
    "    # get the predictions\n",
    "    probas_ = svm_test.predict(X_ttest)\n",
    "            \n",
    "    # turn the predictions into correct format\n",
    "    probas_ = np.array([(1-x, x) for x in probas_])\n",
    "\n",
    "    # create roc curve\n",
    "    probs = svm_test.predict(X_ttest)\n",
    "    preds = probs\n",
    "    fpr, tpr, threshold = metrics.roc_curve(np.ravel(y_ttest), preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    # method I: plt\n",
    "    print(\"AUC: \"+str(roc_auc))\n",
    "    scores.append(roc_auc)\n",
    "print(\"The mean value of the scores is: \", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manolis/anaconda3/envs/tensorflow35/lib/python3.5/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 7.92 seconds for 30 candidates parameter settings.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manolis/anaconda3/envs/tensorflow35/lib/python3.5/site-packages/sklearn/model_selection/_search.py:740: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_bootstrap</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>...</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.120821</td>\n",
       "      <td>0.108009</td>\n",
       "      <td>0.983291</td>\n",
       "      <td>0.984870</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>88</td>\n",
       "      <td>81</td>\n",
       "      <td>...</td>\n",
       "      <td>0.989280</td>\n",
       "      <td>0.983113</td>\n",
       "      <td>0.983506</td>\n",
       "      <td>0.985545</td>\n",
       "      <td>0.977083</td>\n",
       "      <td>0.985953</td>\n",
       "      <td>0.003632</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.004982</td>\n",
       "      <td>0.001254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.115648</td>\n",
       "      <td>0.107668</td>\n",
       "      <td>0.978740</td>\n",
       "      <td>0.980779</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>99</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.983971</td>\n",
       "      <td>0.977826</td>\n",
       "      <td>0.980541</td>\n",
       "      <td>0.981878</td>\n",
       "      <td>0.971704</td>\n",
       "      <td>0.982634</td>\n",
       "      <td>0.005045</td>\n",
       "      <td>0.002097</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>0.002111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.112304</td>\n",
       "      <td>0.108524</td>\n",
       "      <td>0.990211</td>\n",
       "      <td>0.992750</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995138</td>\n",
       "      <td>0.991115</td>\n",
       "      <td>0.990710</td>\n",
       "      <td>0.992800</td>\n",
       "      <td>0.984783</td>\n",
       "      <td>0.994337</td>\n",
       "      <td>0.002527</td>\n",
       "      <td>0.003561</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>0.001316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.119475</td>\n",
       "      <td>0.108963</td>\n",
       "      <td>0.994136</td>\n",
       "      <td>0.997016</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997218</td>\n",
       "      <td>0.996485</td>\n",
       "      <td>0.994208</td>\n",
       "      <td>0.996939</td>\n",
       "      <td>0.990981</td>\n",
       "      <td>0.997626</td>\n",
       "      <td>0.004344</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.000469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.113060</td>\n",
       "      <td>0.109270</td>\n",
       "      <td>0.988484</td>\n",
       "      <td>0.992103</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993248</td>\n",
       "      <td>0.990217</td>\n",
       "      <td>0.989345</td>\n",
       "      <td>0.991238</td>\n",
       "      <td>0.982855</td>\n",
       "      <td>0.994854</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.004287</td>\n",
       "      <td>0.001989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.115207</td>\n",
       "      <td>0.107431</td>\n",
       "      <td>0.993561</td>\n",
       "      <td>0.997147</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>93</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997675</td>\n",
       "      <td>0.996355</td>\n",
       "      <td>0.994633</td>\n",
       "      <td>0.997417</td>\n",
       "      <td>0.988374</td>\n",
       "      <td>0.997669</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.003872</td>\n",
       "      <td>0.000570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.111823</td>\n",
       "      <td>0.108897</td>\n",
       "      <td>0.987300</td>\n",
       "      <td>0.989261</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992420</td>\n",
       "      <td>0.987583</td>\n",
       "      <td>0.989208</td>\n",
       "      <td>0.990021</td>\n",
       "      <td>0.980269</td>\n",
       "      <td>0.990179</td>\n",
       "      <td>0.001613</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>0.001188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.113347</td>\n",
       "      <td>0.107501</td>\n",
       "      <td>0.988762</td>\n",
       "      <td>0.990830</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>81</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993089</td>\n",
       "      <td>0.989636</td>\n",
       "      <td>0.989500</td>\n",
       "      <td>0.991346</td>\n",
       "      <td>0.983694</td>\n",
       "      <td>0.991508</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.001849</td>\n",
       "      <td>0.003871</td>\n",
       "      <td>0.000847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.112933</td>\n",
       "      <td>0.107676</td>\n",
       "      <td>0.986857</td>\n",
       "      <td>0.989106</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>99</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992094</td>\n",
       "      <td>0.987377</td>\n",
       "      <td>0.987171</td>\n",
       "      <td>0.989347</td>\n",
       "      <td>0.981302</td>\n",
       "      <td>0.990596</td>\n",
       "      <td>0.002638</td>\n",
       "      <td>0.001051</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>0.001325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.114020</td>\n",
       "      <td>0.108828</td>\n",
       "      <td>0.989967</td>\n",
       "      <td>0.993075</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994651</td>\n",
       "      <td>0.990889</td>\n",
       "      <td>0.990134</td>\n",
       "      <td>0.993104</td>\n",
       "      <td>0.985114</td>\n",
       "      <td>0.995233</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>0.001774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.113070</td>\n",
       "      <td>0.105646</td>\n",
       "      <td>0.992449</td>\n",
       "      <td>0.994841</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>45</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995885</td>\n",
       "      <td>0.993511</td>\n",
       "      <td>0.993368</td>\n",
       "      <td>0.994829</td>\n",
       "      <td>0.988092</td>\n",
       "      <td>0.996183</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.003247</td>\n",
       "      <td>0.001091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.113773</td>\n",
       "      <td>0.109022</td>\n",
       "      <td>0.983372</td>\n",
       "      <td>0.985149</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>75</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>0.988742</td>\n",
       "      <td>0.982575</td>\n",
       "      <td>0.983906</td>\n",
       "      <td>0.985914</td>\n",
       "      <td>0.977464</td>\n",
       "      <td>0.986960</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>0.001870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.112779</td>\n",
       "      <td>0.108010</td>\n",
       "      <td>0.979467</td>\n",
       "      <td>0.980520</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>54</td>\n",
       "      <td>70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.983815</td>\n",
       "      <td>0.977169</td>\n",
       "      <td>0.979808</td>\n",
       "      <td>0.980587</td>\n",
       "      <td>0.974776</td>\n",
       "      <td>0.983803</td>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.002210</td>\n",
       "      <td>0.003698</td>\n",
       "      <td>0.002709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.112458</td>\n",
       "      <td>0.109346</td>\n",
       "      <td>0.980525</td>\n",
       "      <td>0.983299</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>84</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.984598</td>\n",
       "      <td>0.982108</td>\n",
       "      <td>0.979498</td>\n",
       "      <td>0.983907</td>\n",
       "      <td>0.977477</td>\n",
       "      <td>0.983884</td>\n",
       "      <td>0.002591</td>\n",
       "      <td>0.003091</td>\n",
       "      <td>0.002997</td>\n",
       "      <td>0.000843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.113226</td>\n",
       "      <td>0.109009</td>\n",
       "      <td>0.985016</td>\n",
       "      <td>0.986652</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>77</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.988586</td>\n",
       "      <td>0.984768</td>\n",
       "      <td>0.984691</td>\n",
       "      <td>0.986565</td>\n",
       "      <td>0.981768</td>\n",
       "      <td>0.988622</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.002209</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>0.001575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.113862</td>\n",
       "      <td>0.110895</td>\n",
       "      <td>0.979944</td>\n",
       "      <td>0.983444</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>88</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.984876</td>\n",
       "      <td>0.982360</td>\n",
       "      <td>0.978487</td>\n",
       "      <td>0.983536</td>\n",
       "      <td>0.976466</td>\n",
       "      <td>0.984437</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>0.003585</td>\n",
       "      <td>0.000850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.115173</td>\n",
       "      <td>0.111139</td>\n",
       "      <td>0.992400</td>\n",
       "      <td>0.994917</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>44</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995534</td>\n",
       "      <td>0.993091</td>\n",
       "      <td>0.993236</td>\n",
       "      <td>0.995620</td>\n",
       "      <td>0.988428</td>\n",
       "      <td>0.996041</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.002961</td>\n",
       "      <td>0.001303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.114557</td>\n",
       "      <td>0.110922</td>\n",
       "      <td>0.986075</td>\n",
       "      <td>0.988250</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>95</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>0.991366</td>\n",
       "      <td>0.986895</td>\n",
       "      <td>0.985889</td>\n",
       "      <td>0.987751</td>\n",
       "      <td>0.980967</td>\n",
       "      <td>0.990105</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.001099</td>\n",
       "      <td>0.004247</td>\n",
       "      <td>0.001357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.116028</td>\n",
       "      <td>0.111344</td>\n",
       "      <td>0.983121</td>\n",
       "      <td>0.985225</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>77</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.985830</td>\n",
       "      <td>0.981468</td>\n",
       "      <td>0.982541</td>\n",
       "      <td>0.985291</td>\n",
       "      <td>0.980989</td>\n",
       "      <td>0.988914</td>\n",
       "      <td>0.002621</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.002018</td>\n",
       "      <td>0.003040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.114192</td>\n",
       "      <td>0.107918</td>\n",
       "      <td>0.990887</td>\n",
       "      <td>0.993514</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996203</td>\n",
       "      <td>0.992084</td>\n",
       "      <td>0.992034</td>\n",
       "      <td>0.992978</td>\n",
       "      <td>0.984420</td>\n",
       "      <td>0.995479</td>\n",
       "      <td>0.002115</td>\n",
       "      <td>0.001822</td>\n",
       "      <td>0.004879</td>\n",
       "      <td>0.001437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.114989</td>\n",
       "      <td>0.108772</td>\n",
       "      <td>0.986554</td>\n",
       "      <td>0.988590</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>63</td>\n",
       "      <td>52</td>\n",
       "      <td>...</td>\n",
       "      <td>0.991342</td>\n",
       "      <td>0.986948</td>\n",
       "      <td>0.987126</td>\n",
       "      <td>0.988154</td>\n",
       "      <td>0.981190</td>\n",
       "      <td>0.990667</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.001549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.114074</td>\n",
       "      <td>0.110203</td>\n",
       "      <td>0.987851</td>\n",
       "      <td>0.990151</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>49</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992174</td>\n",
       "      <td>0.988985</td>\n",
       "      <td>0.986906</td>\n",
       "      <td>0.990170</td>\n",
       "      <td>0.984472</td>\n",
       "      <td>0.991297</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.002475</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>0.000944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.127550</td>\n",
       "      <td>0.111366</td>\n",
       "      <td>0.990630</td>\n",
       "      <td>0.993123</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>59</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994925</td>\n",
       "      <td>0.992025</td>\n",
       "      <td>0.990641</td>\n",
       "      <td>0.992851</td>\n",
       "      <td>0.986322</td>\n",
       "      <td>0.994493</td>\n",
       "      <td>0.010735</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.003512</td>\n",
       "      <td>0.001026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.116904</td>\n",
       "      <td>0.108360</td>\n",
       "      <td>0.989921</td>\n",
       "      <td>0.992150</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>67</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993856</td>\n",
       "      <td>0.990869</td>\n",
       "      <td>0.990908</td>\n",
       "      <td>0.992543</td>\n",
       "      <td>0.984997</td>\n",
       "      <td>0.993037</td>\n",
       "      <td>0.004599</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>0.003683</td>\n",
       "      <td>0.000928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.115448</td>\n",
       "      <td>0.108438</td>\n",
       "      <td>0.987436</td>\n",
       "      <td>0.989936</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>90</td>\n",
       "      <td>47</td>\n",
       "      <td>...</td>\n",
       "      <td>0.991782</td>\n",
       "      <td>0.988540</td>\n",
       "      <td>0.987357</td>\n",
       "      <td>0.989454</td>\n",
       "      <td>0.983165</td>\n",
       "      <td>0.991813</td>\n",
       "      <td>0.001477</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.001379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.114599</td>\n",
       "      <td>0.108588</td>\n",
       "      <td>0.988213</td>\n",
       "      <td>0.990501</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>83</td>\n",
       "      <td>69</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992377</td>\n",
       "      <td>0.988111</td>\n",
       "      <td>0.988816</td>\n",
       "      <td>0.990448</td>\n",
       "      <td>0.983442</td>\n",
       "      <td>0.992944</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.001973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.117085</td>\n",
       "      <td>0.109839</td>\n",
       "      <td>0.993953</td>\n",
       "      <td>0.996526</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>91</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997302</td>\n",
       "      <td>0.995640</td>\n",
       "      <td>0.994279</td>\n",
       "      <td>0.996544</td>\n",
       "      <td>0.990277</td>\n",
       "      <td>0.997395</td>\n",
       "      <td>0.005499</td>\n",
       "      <td>0.002313</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>0.000716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.113132</td>\n",
       "      <td>0.108288</td>\n",
       "      <td>0.992175</td>\n",
       "      <td>0.994247</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>95</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995980</td>\n",
       "      <td>0.993019</td>\n",
       "      <td>0.992400</td>\n",
       "      <td>0.994245</td>\n",
       "      <td>0.988142</td>\n",
       "      <td>0.995478</td>\n",
       "      <td>0.002209</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.003204</td>\n",
       "      <td>0.001004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.115264</td>\n",
       "      <td>0.111986</td>\n",
       "      <td>0.990617</td>\n",
       "      <td>0.993212</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994412</td>\n",
       "      <td>0.991813</td>\n",
       "      <td>0.990882</td>\n",
       "      <td>0.993098</td>\n",
       "      <td>0.986554</td>\n",
       "      <td>0.994724</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.003214</td>\n",
       "      <td>0.001191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.114071</td>\n",
       "      <td>0.108127</td>\n",
       "      <td>0.987853</td>\n",
       "      <td>0.991432</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>67</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.991090</td>\n",
       "      <td>0.990109</td>\n",
       "      <td>0.990251</td>\n",
       "      <td>0.991623</td>\n",
       "      <td>0.982216</td>\n",
       "      <td>0.992564</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.001011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0        0.120821         0.108009         0.983291          0.984870   \n",
       "1        0.115648         0.107668         0.978740          0.980779   \n",
       "2        0.112304         0.108524         0.990211          0.992750   \n",
       "3        0.119475         0.108963         0.994136          0.997016   \n",
       "4        0.113060         0.109270         0.988484          0.992103   \n",
       "5        0.115207         0.107431         0.993561          0.997147   \n",
       "6        0.111823         0.108897         0.987300          0.989261   \n",
       "7        0.113347         0.107501         0.988762          0.990830   \n",
       "8        0.112933         0.107676         0.986857          0.989106   \n",
       "9        0.114020         0.108828         0.989967          0.993075   \n",
       "10       0.113070         0.105646         0.992449          0.994841   \n",
       "11       0.113773         0.109022         0.983372          0.985149   \n",
       "12       0.112779         0.108010         0.979467          0.980520   \n",
       "13       0.112458         0.109346         0.980525          0.983299   \n",
       "14       0.113226         0.109009         0.985016          0.986652   \n",
       "15       0.113862         0.110895         0.979944          0.983444   \n",
       "16       0.115173         0.111139         0.992400          0.994917   \n",
       "17       0.114557         0.110922         0.986075          0.988250   \n",
       "18       0.116028         0.111344         0.983121          0.985225   \n",
       "19       0.114192         0.107918         0.990887          0.993514   \n",
       "20       0.114989         0.108772         0.986554          0.988590   \n",
       "21       0.114074         0.110203         0.987851          0.990151   \n",
       "22       0.127550         0.111366         0.990630          0.993123   \n",
       "23       0.116904         0.108360         0.989921          0.992150   \n",
       "24       0.115448         0.108438         0.987436          0.989936   \n",
       "25       0.114599         0.108588         0.988213          0.990501   \n",
       "26       0.117085         0.109839         0.993953          0.996526   \n",
       "27       0.113132         0.108288         0.992175          0.994247   \n",
       "28       0.115264         0.111986         0.990617          0.993212   \n",
       "29       0.114071         0.108127         0.987853          0.991432   \n",
       "\n",
       "   param_bootstrap param_criterion param_max_depth param_max_features  \\\n",
       "0             True            gini               8                  8   \n",
       "1             True            gini              17                  5   \n",
       "2             True            gini              17                  3   \n",
       "3             True         entropy              11                 11   \n",
       "4            False            gini               5                 12   \n",
       "5            False         entropy              14                  4   \n",
       "6            False         entropy              20                  2   \n",
       "7            False            gini              20                  8   \n",
       "8            False         entropy              17                  4   \n",
       "9            False         entropy            None                  2   \n",
       "10           False         entropy              14                  7   \n",
       "11            True            gini               5                  6   \n",
       "12            True         entropy               5                  2   \n",
       "13            True            gini               8                 12   \n",
       "14            True            gini              14                 10   \n",
       "15            True         entropy              11                 11   \n",
       "16           False         entropy            None                  5   \n",
       "17           False         entropy              17                  7   \n",
       "18            True         entropy               5                  8   \n",
       "19            True            gini               5                  7   \n",
       "20            True         entropy              14                  5   \n",
       "21            True         entropy              20                 10   \n",
       "22           False            gini              17                  6   \n",
       "23           False            gini               8                 10   \n",
       "24           False         entropy              14                  6   \n",
       "25           False            gini              11                  7   \n",
       "26            True         entropy              11                 11   \n",
       "27            True            gini              11                  6   \n",
       "28           False            gini              11                  6   \n",
       "29           False            gini              17                 13   \n",
       "\n",
       "   param_min_samples_leaf param_min_samples_split       ...         \\\n",
       "0                      88                      81       ...          \n",
       "1                      99                      55       ...          \n",
       "2                      25                      17       ...          \n",
       "3                      17                      11       ...          \n",
       "4                       5                      33       ...          \n",
       "5                       5                      93       ...          \n",
       "6                      48                      67       ...          \n",
       "7                      81                       4       ...          \n",
       "8                      99                      65       ...          \n",
       "9                      31                      50       ...          \n",
       "10                     45                      20       ...          \n",
       "11                     75                      19       ...          \n",
       "12                     54                      70       ...          \n",
       "13                     84                      55       ...          \n",
       "14                     77                       5       ...          \n",
       "15                     88                      62       ...          \n",
       "16                     44                      38       ...          \n",
       "17                     95                      44       ...          \n",
       "18                     77                      35       ...          \n",
       "19                      1                      57       ...          \n",
       "20                     63                      52       ...          \n",
       "21                     49                      23       ...          \n",
       "22                     59                      41       ...          \n",
       "23                     67                      26       ...          \n",
       "24                     90                      47       ...          \n",
       "25                     83                      69       ...          \n",
       "26                      9                      91       ...          \n",
       "27                     19                      95       ...          \n",
       "28                     58                       2       ...          \n",
       "29                     67                      18       ...          \n",
       "\n",
       "    split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0            0.989280            0.983113           0.983506   \n",
       "1            0.983971            0.977826           0.980541   \n",
       "2            0.995138            0.991115           0.990710   \n",
       "3            0.997218            0.996485           0.994208   \n",
       "4            0.993248            0.990217           0.989345   \n",
       "5            0.997675            0.996355           0.994633   \n",
       "6            0.992420            0.987583           0.989208   \n",
       "7            0.993089            0.989636           0.989500   \n",
       "8            0.992094            0.987377           0.987171   \n",
       "9            0.994651            0.990889           0.990134   \n",
       "10           0.995885            0.993511           0.993368   \n",
       "11           0.988742            0.982575           0.983906   \n",
       "12           0.983815            0.977169           0.979808   \n",
       "13           0.984598            0.982108           0.979498   \n",
       "14           0.988586            0.984768           0.984691   \n",
       "15           0.984876            0.982360           0.978487   \n",
       "16           0.995534            0.993091           0.993236   \n",
       "17           0.991366            0.986895           0.985889   \n",
       "18           0.985830            0.981468           0.982541   \n",
       "19           0.996203            0.992084           0.992034   \n",
       "20           0.991342            0.986948           0.987126   \n",
       "21           0.992174            0.988985           0.986906   \n",
       "22           0.994925            0.992025           0.990641   \n",
       "23           0.993856            0.990869           0.990908   \n",
       "24           0.991782            0.988540           0.987357   \n",
       "25           0.992377            0.988111           0.988816   \n",
       "26           0.997302            0.995640           0.994279   \n",
       "27           0.995980            0.993019           0.992400   \n",
       "28           0.994412            0.991813           0.990882   \n",
       "29           0.991090            0.990109           0.990251   \n",
       "\n",
       "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0             0.985545           0.977083            0.985953      0.003632   \n",
       "1             0.981878           0.971704            0.982634      0.005045   \n",
       "2             0.992800           0.984783            0.994337      0.002527   \n",
       "3             0.996939           0.990981            0.997626      0.004344   \n",
       "4             0.991238           0.982855            0.994854      0.001792   \n",
       "5             0.997417           0.988374            0.997669      0.006823   \n",
       "6             0.990021           0.980269            0.990179      0.001613   \n",
       "7             0.991346           0.983694            0.991508      0.000699   \n",
       "8             0.989347           0.981302            0.990596      0.002638   \n",
       "9             0.993104           0.985114            0.995233      0.000551   \n",
       "10            0.994829           0.988092            0.996183      0.002199   \n",
       "11            0.985914           0.977464            0.986960      0.002004   \n",
       "12            0.980587           0.974776            0.983803      0.000829   \n",
       "13            0.983907           0.977477            0.983884      0.002591   \n",
       "14            0.986565           0.981768            0.988622      0.002646   \n",
       "15            0.983536           0.976466            0.984437      0.000565   \n",
       "16            0.995620           0.988428            0.996041      0.001080   \n",
       "17            0.987751           0.980967            0.990105      0.001203   \n",
       "18            0.985291           0.980989            0.988914      0.002621   \n",
       "19            0.992978           0.984420            0.995479      0.002115   \n",
       "20            0.988154           0.981190            0.990667      0.001128   \n",
       "21            0.990170           0.984472            0.991297      0.002273   \n",
       "22            0.992851           0.986322            0.994493      0.010735   \n",
       "23            0.992543           0.984997            0.993037      0.004599   \n",
       "24            0.989454           0.983165            0.991813      0.001477   \n",
       "25            0.990448           0.983442            0.992944      0.000504   \n",
       "26            0.996544           0.990277            0.997395      0.005499   \n",
       "27            0.994245           0.988142            0.995478      0.002209   \n",
       "28            0.993098           0.986554            0.994724      0.001098   \n",
       "29            0.991623           0.982216            0.992564      0.000835   \n",
       "\n",
       "    std_score_time  std_test_score  std_train_score  \n",
       "0         0.001031        0.004982         0.001254  \n",
       "1         0.002097        0.005168         0.002111  \n",
       "2         0.003561        0.004242         0.001316  \n",
       "3         0.002867        0.002547         0.000469  \n",
       "4         0.001313        0.004287         0.001989  \n",
       "5         0.001370        0.003872         0.000570  \n",
       "6         0.001317        0.005141         0.001188  \n",
       "7         0.001849        0.003871         0.000847  \n",
       "8         0.001051        0.004412         0.001325  \n",
       "9         0.003346        0.003895         0.001774  \n",
       "10        0.000333        0.003247         0.001091  \n",
       "11        0.001629        0.004620         0.001870  \n",
       "12        0.002210        0.003698         0.002709  \n",
       "13        0.003091        0.002997         0.000843  \n",
       "14        0.002209        0.002793         0.001575  \n",
       "15        0.001168        0.003585         0.000850  \n",
       "16        0.000112        0.002961         0.001303  \n",
       "17        0.001099        0.004247         0.001357  \n",
       "18        0.000590        0.002018         0.003040  \n",
       "19        0.001822        0.004879         0.001437  \n",
       "20        0.000831        0.004164         0.001549  \n",
       "21        0.002475        0.003215         0.000944  \n",
       "22        0.000170        0.003512         0.001026  \n",
       "23        0.002072        0.003683         0.000928  \n",
       "24        0.001989        0.003518         0.001379  \n",
       "25        0.000642        0.003673         0.001973  \n",
       "26        0.002313        0.002878         0.000716  \n",
       "27        0.002632        0.003204         0.001004  \n",
       "28        0.000602        0.003214         0.001191  \n",
       "29        0.002773        0.004000         0.001011  \n",
       "\n",
       "[30 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "bootstrap               True\n",
       "criterion            entropy\n",
       "max_depth                 11\n",
       "max_features              11\n",
       "min_samples_leaf          17\n",
       "min_samples_split         11\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyperparameter Optimization for random forest\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "from time import time\n",
    "\n",
    "# initialize the classifier\n",
    "clf_RF = RandomForestClassifier(n_estimators=10, n_jobs=-1, random_state=100)\n",
    "\n",
    "# find the number of features for parameter optimization\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "# make scorer\n",
    "auc_scorer = make_scorer(roc_auc_score, needs_threshold=True)#, needs_proba=True\n",
    "\n",
    "# params\n",
    "m_depths = [None] + list(range(2, 21, 3))\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist_rf = {\"max_depth\": m_depths, # [None, 2, 5, 8, 11, 14, 17, 20]\n",
    "                 \"max_features\": sp_randint(1, num_features),\n",
    "                 \"min_samples_split\": sp_randint(2, 100),\n",
    "                 \"min_samples_leaf\": sp_randint(1, 100),\n",
    "                 \"bootstrap\": [True, False],\n",
    "                 \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "n_iter_search = 30 # was 20\n",
    "\n",
    "# run randomized search\n",
    "random_search_RF = RandomizedSearchCV(clf_RF, \n",
    "                                      param_distributions=param_dist_rf,\n",
    "                                      n_iter=n_iter_search,\n",
    "                                      scoring=auc_scorer,\n",
    "                                      random_state=random_state,\n",
    "                                      n_jobs=-1,\n",
    "                                      return_train_score=True)\n",
    "\n",
    "start = time()\n",
    "random_search_RF.fit(X_train, Y_train)\n",
    "print(\"RandomizedSearchCV took {:.2f} seconds for {} candidates\"\n",
    "      \" parameter settings.\\n\".format((time() - start), n_iter_search))\n",
    "\n",
    "# display the dataframe without the parameters column for better rendering\n",
    "display(pd.DataFrame(random_search_RF.cv_results_).drop('params', axis=1))\n",
    "\n",
    "best_parameters_RF = pd.Series(random_search_RF.best_params_)\n",
    "\n",
    "# display the best set of parameters\n",
    "display(best_parameters_RF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manolis/anaconda3/envs/tensorflow35/lib/python3.5/site-packages/ipykernel_launcher.py:11: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.979955039340577\n",
      "test accuracy: 0.9729020979020979\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Train optimized random forest\n",
    "model7 = RandomForestClassifier(n_estimators=10 ,\n",
    "                                min_samples_split= 11,\n",
    "                                min_samples_leaf=17,\n",
    "                                bootstrap = True,\n",
    "                                criterion = 'entropy',\n",
    "                                max_depth = 11,\n",
    "                                max_features = 11,\n",
    "                                n_jobs=-1,\n",
    "                                random_state=100)\n",
    "model7.fit(X_train, Y_train)\n",
    "print('train accuracy: '+str(jaccard_similarity_score(model7.predict(X_train), Y_train)))\n",
    "print('test accuracy: '+str(jaccard_similarity_score(model7.predict(X_test), Y_test)))\n",
    "\n",
    "# create roc curve\n",
    "probs = model7.predict(X_test)\n",
    "preds = probs\n",
    "fpr, tpr, threshold = metrics.roc_curve(Y_test, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# method I: plt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest global feature importance\n",
    "Find the most important features in the whole dataset (including train and test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "importances = model7.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in model7.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(data.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(data.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(data.shape[1]), indices)\n",
    "plt.xlim([-1, data.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get contributing features in a (RF) signle prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "#eli5.show_prediction(model7, selected_example, feature_names=list(data))\n",
    "eli5.explain_prediction(model7, selected_example, feature_names=list(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "best_params = {\"eta\": None,\n",
    "               \"min_child_weight\": None,\n",
    "               \"gamma\": None,\n",
    "               \"max_depth\": None,\n",
    "               \"max_delta_step\": None,\n",
    "               \"subsample\": None,\n",
    "               \"colsample_bytree\": None,\n",
    "               \"lambda\": None,\n",
    "               \"objective\": None}\n",
    "\n",
    "number_of_iterations = {\"after_tree_params\": None,\n",
    "                        \"after_reg_params\": None,\n",
    "                        \"after_function_param\": None,\n",
    "                        \"after_eta\": None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(random_state)\n",
    "\n",
    "best_score = 0.0\n",
    "\n",
    "num_iterations = 20\n",
    "\n",
    "max_iterations = []\n",
    "\n",
    "# get data into the correct format\n",
    "xgbData = xgb.DMatrix(X_train, label=Y_train)\n",
    "\n",
    "# prepare parameters\n",
    "maxDepths = np.random.randint(3, 40, size=num_iterations)\n",
    "minChildWeights = np.random.randint(0, 40, size=num_iterations)\n",
    "gammas = np.random.randint(3, 50, size=num_iterations)\n",
    "maxDeltaStep = np.random.randint(0, 10, size=num_iterations)\n",
    "\n",
    "params_tupled = [(a, b, c, d) for a, b, c, d in zip(maxDepths,\n",
    "                                                    minChildWeights,\n",
    "                                                    gammas,\n",
    "                                                    maxDeltaStep)]\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist_XGB = {\"eta\": 0.3,\n",
    "                  \"min_child_weight\": 1.0,\n",
    "                  \"gamma\": 0.0,\n",
    "                  \"max_depth\": 20,\n",
    "                  \"max_delta_step\": 0.0,\n",
    "                  \"subsample\": 0.8,\n",
    "                  \"colsample_bytree\": 0.8,\n",
    "                  \"lambda\": 1,\n",
    "                  \"objective\": \"binary:logistic\"}\n",
    "\n",
    "for mD, mCW ,g ,mDS in params_tupled:\n",
    "    \n",
    "    param_dist_XGB[\"max_depth\"] = mD\n",
    "    param_dist_XGB[\"min_child_weight\"] = mCW\n",
    "    param_dist_XGB[\"gamma\"] = g\n",
    "    param_dist_XGB[\"max_delta_step\"] = mDS\n",
    "\n",
    "    # get a cross validated result\n",
    "    cv_XGB = xgb.cv(param_dist_XGB,\n",
    "                    xgbData,\n",
    "                    num_boost_round=300,\n",
    "                    seed=random_state,\n",
    "                    nfold=5,\n",
    "                    stratified=True,\n",
    "                    metrics={'auc'},\n",
    "                    early_stopping_rounds=30,\n",
    "                    as_pandas=True,\n",
    "                    shuffle=False)\n",
    "    \n",
    "    # get the smallest score\n",
    "    mean_auc_test_score = cv_XGB['test-auc-mean'].max()\n",
    "    \n",
    "    print(\"The highest auc score of the cross validation is {}.\".format(mean_auc_test_score))\n",
    "    \n",
    "    if mean_auc_test_score < 0.85:\n",
    "        print(\" -> its params were: max_depth={}, min_child_weight={}, gamma={}, max_delta_step={}\"\n",
    "              .format(mD, mCW, g, mDS))\n",
    "    \n",
    "    max_iterations.append(cv_XGB.index.values[-1] + 1)\n",
    "    \n",
    "    if mean_auc_test_score > best_score:\n",
    "        \n",
    "        # update the smallest error\n",
    "        best_score = mean_auc_test_score\n",
    "        # update best parameters\n",
    "        best_params[\"max_depth\"] = mD\n",
    "        best_params[\"min_child_weight\"] = mCW\n",
    "        best_params[\"gamma\"] = g\n",
    "        best_params[\"max_delta_step\"] = mDS\n",
    "\n",
    "number_of_iterations[\"after_tree_params\"] = max(max_iterations)\n",
    "\n",
    "print()\n",
    "print(\"The optimal max_depth was found to be: {}\".format(best_params[\"max_depth\"]))\n",
    "print(\"The optimal min_child_weight was found to be: {}\".format(best_params[\"min_child_weight\"]))\n",
    "print(\"The optimal gamma was found to be: {}\".format(best_params[\"gamma\"]))\n",
    "print(\"The optimal max_delta_step was found to be: {}\\n\".format(best_params[\"max_delta_step\"]))\n",
    "\n",
    "print(\"The best auc score for the best tree parameters is {}.\\n\".format(best_score))\n",
    "\n",
    "print(\"The number of iterations for the train cases were {}, and the max of them is {}.\"\n",
    "      .format(max_iterations, number_of_iterations[\"after_tree_params\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0.0\n",
    "\n",
    "num_iterations = 20\n",
    "\n",
    "max_iterations = []\n",
    "\n",
    "# prepare parameters\n",
    "subsampleParam = np.random.uniform(0.1, 1, size=num_iterations)\n",
    "colsample_bytreeParam = np.random.uniform(0.1, 1, size=num_iterations)\n",
    "lambdaParam = np.random.randint(1, 5, size=num_iterations)\n",
    "\n",
    "params_tupled = [(a, b, c) for a, b, c in zip(subsampleParam,\n",
    "                                              colsample_bytreeParam,\n",
    "                                              lambdaParam)]\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist_XGB = {\"eta\": 0.3,\n",
    "                  \"min_child_weight\": best_params[\"min_child_weight\"],\n",
    "                  \"gamma\": best_params[\"gamma\"],\n",
    "                  \"max_depth\": best_params[\"max_depth\"],\n",
    "                  \"max_delta_step\": best_params[\"max_delta_step\"],\n",
    "                  \"subsample\": 0.8,\n",
    "                  \"colsample_bytree\": 0.8,\n",
    "                  \"lambda\": 1,\n",
    "                  \"objective\": \"binary:logistic\"}\n",
    "\n",
    "for ss, csbt ,l in params_tupled:\n",
    "    \n",
    "    param_dist_XGB[\"subsample\"] = ss\n",
    "    param_dist_XGB[\"colsample_bytree\"] = csbt\n",
    "    param_dist_XGB[\"lambda\"] = l\n",
    "\n",
    "    # get a cross validated result\n",
    "    cv_XGB = xgb.cv(param_dist_XGB,\n",
    "                    xgbData,\n",
    "                    num_boost_round=300,\n",
    "                    seed=random_state,\n",
    "                    nfold=5,\n",
    "                    stratified=True,\n",
    "                    metrics={'auc'},\n",
    "                    early_stopping_rounds=30,\n",
    "                    as_pandas=True,\n",
    "                    shuffle=False)\n",
    "    \n",
    "    # get the smallest score\n",
    "    mean_auc_test_score = cv_XGB['test-auc-mean'].max()\n",
    "    \n",
    "    print(\"The smallest auc score of the cross validation is {}.\".format(mean_auc_test_score))\n",
    "    \n",
    "    if mean_auc_test_score < 0.85:\n",
    "        print(\"  -> its parameters were: subsample={}, colsample_bytree={}, lambda={}.\"\n",
    "              .format(ss, csbt, l))\n",
    "        \n",
    "    max_iterations.append(cv_XGB.index.values[-1] + 1)\n",
    "    \n",
    "    if mean_auc_test_score > best_score:\n",
    "        \n",
    "        # update the smallest error\n",
    "        best_score = mean_auc_test_score\n",
    "        # update best parameters\n",
    "        best_params[\"subsample\"] = ss\n",
    "        best_params[\"colsample_bytree\"] = csbt\n",
    "        best_params[\"lambda\"] = l\n",
    "\n",
    "number_of_iterations[\"after_reg_params\"] = max(max_iterations)\n",
    "\n",
    "print()\n",
    "print(\"The optimal subsample was found to be: {}\".format(best_params[\"subsample\"]))\n",
    "print(\"The optimal colsample_bytree was found to be: {}\".format(best_params[\"colsample_bytree\"]))\n",
    "print(\"The optimal lambda was found to be: {}\\n\".format(best_params[\"lambda\"]))\n",
    "\n",
    "print(\"The best auc score for the best tree parameters is {}.\\n\".format(best_score))\n",
    "\n",
    "print(\"The number of iterations for the train cases were {}, and the max of them is {}.\"\n",
    "      .format(max_iterations, number_of_iterations[\"after_reg_params\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0.0\n",
    "\n",
    "max_iterations = []\n",
    "\n",
    "params_tupled = [\"binary:logistic\", \"binary:logitraw\"]\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist_XGB = {\"eta\": 0.3,\n",
    "                  \"min_child_weight\": best_params[\"min_child_weight\"],\n",
    "                  \"gamma\": best_params[\"gamma\"],\n",
    "                  \"max_depth\": best_params[\"max_depth\"],\n",
    "                  \"max_delta_step\": best_params[\"max_delta_step\"],\n",
    "                  \"subsample\": best_params[\"subsample\"],\n",
    "                  \"colsample_bytree\": best_params[\"colsample_bytree\"],\n",
    "                  \"lambda\": best_params[\"lambda\"],\n",
    "                  \"objective\": None}\n",
    "\n",
    "for func in params_tupled:\n",
    "    \n",
    "    param_dist_XGB[\"objective\"] = func\n",
    "\n",
    "    # get a cross validated result\n",
    "    cv_XGB = xgb.cv(param_dist_XGB,\n",
    "                    xgbData,\n",
    "                    num_boost_round=300,\n",
    "                    seed=random_state,\n",
    "                    nfold=5,\n",
    "                    stratified=True,\n",
    "                    metrics={'auc'},\n",
    "                    early_stopping_rounds=30,\n",
    "                    as_pandas=True,\n",
    "                    shuffle=False)\n",
    "    \n",
    "    # get the smallest score\n",
    "    mean_auc_test_score = cv_XGB['test-auc-mean'].max()\n",
    "    \n",
    "    print(\"The smallest auc score of the cross validation is {}.\".format(mean_auc_test_score))\n",
    "    \n",
    "    if mean_auc_test_score < 0.85:\n",
    "        print(\"  -> its parameters were: function={}.\"\n",
    "              .format(func))\n",
    "        \n",
    "    max_iterations.append(cv_XGB.index.values[-1] + 1)\n",
    "    \n",
    "    if mean_auc_test_score > best_score:\n",
    "        \n",
    "        # update the smallest error\n",
    "        best_score = mean_auc_test_score\n",
    "        # update best parameters\n",
    "        best_params[\"objective\"] = func\n",
    "\n",
    "number_of_iterations[\"after_function_param\"] = max(max_iterations)\n",
    "\n",
    "print()\n",
    "print(\"The optimal function was found to be: {}\\n\".format(best_params[\"objective\"]))\n",
    "\n",
    "print(\"The best auc score for the best tree parameters is {}.\".format(best_score))\n",
    "\n",
    "print(\"The number of iterations for the train cases were {}, and the max of them is {}.\"\n",
    "      .format(max_iterations, number_of_iterations[\"after_function_param\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0.0\n",
    "\n",
    "max_iterations = []\n",
    "\n",
    "#params_tupled = np.arange(0.005, 0.5, 0.02)\n",
    "params_tupled = np.arange(0.05, 1.0, 0.02)\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist_XGB = {\"eta\": 0.1,\n",
    "                  \"min_child_weight\": best_params[\"min_child_weight\"],\n",
    "                  \"gamma\": best_params[\"gamma\"],\n",
    "                  \"max_depth\": best_params[\"max_depth\"],\n",
    "                  \"max_delta_step\": best_params[\"max_delta_step\"],\n",
    "                  \"subsample\": best_params[\"subsample\"],\n",
    "                  \"colsample_bytree\": best_params[\"colsample_bytree\"],\n",
    "                  \"lambda\": best_params[\"lambda\"],\n",
    "                  \"objective\": best_params[\"objective\"]}\n",
    "\n",
    "for eta in params_tupled:\n",
    "    \n",
    "    param_dist_XGB[\"eta\"] = eta\n",
    "\n",
    "    # get a cross validated result\n",
    "    cv_XGB = xgb.cv(param_dist_XGB,\n",
    "                    xgbData,\n",
    "                    num_boost_round=300,\n",
    "                    seed=random_state,\n",
    "                    nfold=5,\n",
    "                    stratified=True,\n",
    "                    metrics={'auc'},\n",
    "                    early_stopping_rounds=30,\n",
    "                    as_pandas=True,\n",
    "                    shuffle=False)\n",
    "    \n",
    "    # get the smallest score\n",
    "    mean_auc_test_score = cv_XGB['test-auc-mean'].max()\n",
    "    \n",
    "    print(\"The smallest auc score of the cross validation is {}.\".format(mean_auc_test_score))\n",
    "    \n",
    "    if mean_auc_test_score < 0.85:\n",
    "        print(\"The eta was {}.\".format(eta))\n",
    "        \n",
    "    max_iterations.append(cv_XGB.index.values[-1] + 1)\n",
    "    \n",
    "    if mean_auc_test_score > best_score:\n",
    "        \n",
    "        # update the smallest error\n",
    "        best_score = mean_auc_test_score\n",
    "        # update best parameters\n",
    "        best_params[\"eta\"] = eta\n",
    "\n",
    "number_of_iterations[\"after_eta\"] = max(max_iterations)\n",
    "\n",
    "print()\n",
    "print(\"The eta parameter was found to be: {}\\n\".format(best_params[\"eta\"]))\n",
    "\n",
    "print(\"The best auc score for the best tree parameters is {}.\".format(best_score))\n",
    "\n",
    "print(\"The number of iterations for the train cases were {}, and the max of them is {}.\"\n",
    "      .format(max_iterations, number_of_iterations[\"after_eta\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf_XGB = xgb\n",
    "print(\"The best XGB classifier is:\\n\\n{}\\n\".format(best_clf_XGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best set of parameters for the XGB classifier are:\\n\")\n",
    "\n",
    "display(pd.Series(best_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interp\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import time\n",
    "np.set_printoptions(threshold=400)\n",
    "\n",
    "def make_and_plot_AUROC(clf, XX, yy, n_folds=10, RF=True, shuffle=True, save_name=''):\n",
    "    \"\"\" Calculates the mean AUROC and plots it.\"\"\"\n",
    "\n",
    "    #start = time()\n",
    "    \n",
    "    # initialize the cross validation object for 10 folds\n",
    "    cv = StratifiedKFold(n_splits=n_folds, shuffle=shuffle, random_state=0)\n",
    "\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    \n",
    "    feature_scores = []\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    fig.set_figwidth(8)\n",
    "    fig.set_figheight(6)\n",
    "\n",
    "    i = 0\n",
    "    for train, test in cv.split(XX, yy):\n",
    "        \n",
    "        if RF:\n",
    "            probas_ = clf.fit(XX[train], yy[train]).predict_proba(XX[test])\n",
    "        else:\n",
    "            # prep the data\n",
    "            data_train = xgb.DMatrix(XX[train], label=yy[train])\n",
    "            data_test = xgb.DMatrix(XX[test], label=yy[test])\n",
    "                        \n",
    "            # get the booster instance after training\n",
    "            model = clf.train(best_params, data_train, \n",
    "                              number_of_iterations[\"after_function_param\"]+5)\n",
    "            \n",
    "            # get the predictions\n",
    "            probas_ = model.predict(data_test)\n",
    "            \n",
    "            # turn the predictions into correct format\n",
    "            probas_ = np.array([(1-x, x) for x in probas_])\n",
    "            \n",
    "        # Compute ROC curve and area under the curve\n",
    "        fpr, tpr, thresholds = roc_curve(yy[test], probas_[:, 1], pos_label=1)\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "        ax.plot(fpr, tpr, lw=1, alpha=0.3,\n",
    "                 label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "        i += 1\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "             label='Luck', alpha=.8)\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "             label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "             lw=2, alpha=.8)\n",
    "\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                     label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    ax.set_xlim([-0.05, 1.05])\n",
    "    ax.set_ylim([-0.05, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('AUROC curves of the {} folds and their mean curve.'.format(n_folds))\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    if save_name:\n",
    "        pass\n",
    "        #plt.savefig('images/ROC visualization of {} for {} folds.png'.format(save_name, n_folds), \n",
    "                    #bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"The full auc score is {}.\".format(mean_auc))\n",
    "    print(\"The std of the mean auc score is {}.\\n\".format(std_auc))\n",
    "    \n",
    "    #print(\"The time it took for making and plotting the mean curve is {}.\".format(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use make_and_plot_AUROC with XGB\n",
    "make_and_plot_AUROC(best_clf_XGB, X_train, Y_train, RF=False, shuffle=False, save_name='XGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
